diff --git a/pytorch_lightning_distributed_training/dataloader.py b/pytorch_lightning_distributed_training/dataloader.py
index 5329159..ce835af 100644
--- a/pytorch_lightning_distributed_training/dataloader.py
+++ b/pytorch_lightning_distributed_training/dataloader.py
@@ -19,7 +19,7 @@ class synthetic_imagenet_test(Dataset):
         return image, label 
 
 class ImageNetDataModule(LightningDataModule): 
-    def __init__(self, data_path: Optional[str]=None, 
+    def __init__(self, data_path: Optional[str]="/data1/1K_New/", 
                 batch_size: int =4, 
                 workers: int= 2, 
                 **kwargs, 
diff --git a/pytorch_lightning_distributed_training/train.py b/pytorch_lightning_distributed_training/train.py
index 13c61ae..0080e15 100644
--- a/pytorch_lightning_distributed_training/train.py
+++ b/pytorch_lightning_distributed_training/train.py
@@ -22,19 +22,41 @@ from dataloader import ImageNetDataModule
 from model import ImageNetLightningModel 
 #https://pytorch-lightning.readthedocs.io/en/stable/_modules/pytorch_lightning/utilities/cli.html
 from pytorch_lightning.utilities.cli import LightningCLI
+from pytorch_lightning import Trainer, seed_everything
+
+from pytorch_lightning.loggers import WandbLogger
 
 def main(): 
-    # """Implementation of a configurable command line tool for pytorch-lightning."""
-    cli= LightningCLI(
-       description="Py-Lightning Distributed multi-node training", 
-       model_class= ImageNetLightningModel,
-       datamodule_class= ImageNetDataModule,
-       seed_everything_default=123, 
-       trainer_defaults=dict(accelerator="ddb", max_epoch=10)  
+    seed_everything(123) 
+    model = ImageNetLightningModel()
+    wandb_logger = WandbLogger(
+    name='test-node1',
+    project='Multi-node-training',
+    entity="tranrick",
+    offline= False, #args.offline,
+    group = 'testing-machine',
+    job_type ='conda envs'
     )
-    # TODO: determine per-process batch size given total batch size
-    # TODO: enable evaluate
-    cli.trainer.fit(cli.model, datamodule=cli.datamodule)
+    wandb_logger.watch(model, log="gradients", log_freq=100)
+    # """Implementation of a configurable command line tool for pytorch-lightning."""
+    # cli= LightningCLI(
+    #    description="Py-Lightning Distributed multi-node training", 
+    #    model_class= ImageNetLightningModel,
+    #    datamodule_class= ImageNetDataModule,
+    #    seed_everything_default=123, 
+    #    trainer_defaults=dict(accelerator="ddb", max_epochs=10, gpus=[1], num_nodes=2)  
+    # )
+    # # TODO: determine per-process batch size given total batch size
+    # # TODO: enable evaluate
+    # cli.trainer.fit(cli.model, datamodule=cli.datamodule)
+  
+    dataloader=ImageNetLightningModel()
+    train_loader=dataloader.train_dataloader
+    val_loader=dataloader.val_dataloader
+    # cli.trainer.fit(cli.model, datamodule=cli.datamodule)
+    trainer= Trainer(strategy="ddp", max_epochs=10, gpus=2,  num_nodes=2, logger=wandb_logger)
+    trainer.fit(model, train_loader, val_loader)
+
 
 if __name__ == "__main__": 
     main() 
